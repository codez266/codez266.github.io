<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Supporting Wikipedia's manual review pipeline through personalized AI-assisted topic triaging. | Sumit Asthana </title> <meta name="author" content="Sumit Asthana"> <meta name="description" content="Using topic prediction to reduce review backlog and support expert routing"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="sumitasthana.xyz/projects/project_drafttopic/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sumit</span> Asthana </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Supporting Wikipedia's manual review pipeline through personalized AI-assisted topic triaging.</h1> <p class="post-description">Using topic prediction to reduce review backlog and support expert routing</p> </header> <article> <blockquote> <p>üìò <strong>This post is an accessible read of the publication:</strong></p> <p>Asthana, Sumit, and Aaron Halfaker. <em>‚ÄúWith few eyes, all hoaxes are deep.‚Äù</em> <em>Proceedings of the ACM on Human-Computer Interaction</em>, 2.CSCW (2018): 1‚Äì18. <a href="https://doi.org/10.1145/3274283" rel="external nofollow noopener" target="_blank">https://doi.org/10.1145/3274283</a></p> </blockquote> <p>This blog version summarizes the core motivation, data science process, and findings in a more readable, less academic format. If you‚Äôre curious about Wikipedia workflows, AI-assisted quality control, or just like cool applications of machine learning in collaborative communities ‚Äî read on!</p> <p>Wikipedia flips the traditional publication model: publish first, review later. But with thousands of new articles created each month, the platform faces a growing challenge ‚Äî how to efficiently review submissions and ensure that only encyclopedic content survives.</p> <p>This project tackles a critical bottleneck in that process: <strong>Time-Consuming Judgment Calls (TCJCs)</strong> ‚Äî the nuanced reviews that require expertise to determine if an article subject is notable enough. These articles aren‚Äôt obvious spam or vandalism, but they‚Äôre not yet ready for the front page either.</p> <h3 id="the-review-crisis">The Review Crisis</h3> <p>Two groups primarily handle new article review:</p> <ul> <li> <strong>New Page Patrol (NPP)</strong> ‚Äî reviews articles created directly in the main encyclopedia</li> <li> <strong>Articles for Creation (AfC)</strong> ‚Äî handles submissions in the draft namespace</li> </ul> <p>Both groups rely on volunteers, and both are overwhelmed. NPP and AfC maintain massive backlogs, and recent proposals to limit article creation by new editors only shift the burden ‚Äî they don‚Äôt solve the problem. Worse, routing newcomers to hidden drafts reduces collaboration and delays quality content.</p> <p>In contrast, <strong>edit review</strong> on Wikipedia is highly efficient:</p> <ul> <li>Stage 1: Bots filter out clear damage</li> <li>Stage 2: AI-assisted tools help catch borderline edits</li> <li>Stage 3: Human editors review through personalized watchlists</li> </ul> <p>There‚Äôs no comparable multi-stage system for new articles ‚Äî just one layer of triage by a small group of humans.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wikipedia_drattopic_intro-480.webp 480w,/assets/img/wikipedia_drattopic_intro-800.webp 800w,/assets/img/wikipedia_drattopic_intro-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/wikipedia_drattopic_intro.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Overview of Wikipedia review processes" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of Wikipedia review processes for drafts and ensuring article quality. </div> <hr> <h3 id="our-solution-predict-topics-to-route-reviewers">Our Solution: Predict Topics to Route Reviewers</h3> <p>We propose building an AI-augmented pipeline for new article review ‚Äî starting with a <strong>topic prediction model</strong> to help match articles with subject-matter experts early in the review process.</p> <h4 id="key-idea">Key idea</h4> <p>Instead of waiting for editors to manually tag drafts with topics (often inconsistently), we use Wikipedia‚Äôs <strong>folksonomic structure</strong> ‚Äî WikiProjects and their broad categories ‚Äî to train a model that assigns drafts to topic areas automatically.</p> <h3 id="dataset-drafttopic">Dataset: <code class="language-plaintext highlighter-rouge">drafttopic</code> </h3> <p>We created a large-scale open dataset of new Wikipedia drafts using the <code class="language-plaintext highlighter-rouge">drafttopic</code> API and a custom WikiProjects parser. This dataset allows for:</p> <ul> <li>Multi-label classification over 43 broad WikiProject categories</li> <li>Dynamic updates as WikiProjects evolve</li> <li>Tagging of new and historical drafts at scale</li> </ul> <hr> <h3 id="wikipedia-dump-processing--draft-extraction">Wikipedia Dump Processing &amp; Draft Extraction</h3> <p>Our pipeline begins with parsing the full monthly <strong>Wikipedia XML dumps</strong> (100+ GB compressed) to extract all articles in the <strong>Draft namespace</strong>. We use a scalable SAX-style parser (<code class="language-plaintext highlighter-rouge">mwxml</code>) to stream pages and filter them with:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">page.namespace == 118</code> (Draft namespace)</li> <li> <code class="language-plaintext highlighter-rouge">revision.text</code> present (non-empty articles)</li> <li> <code class="language-plaintext highlighter-rouge">title</code> does not contain ‚Äú/‚Äù (exclude user sandbox subpages)</li> </ul> <p>We then enrich each article with metadata like creation timestamp, creator, and revision history.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">mwxml</span>

<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">mwxml</span><span class="p">.</span><span class="n">Dump</span><span class="p">.</span><span class="nf">from_file</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">enwiki-latest-pages-meta-history.xml.bz2</span><span class="sh">'</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">page</span><span class="p">.</span><span class="n">namespace</span> <span class="o">==</span> <span class="mi">118</span> <span class="ow">and</span> <span class="ow">not</span> <span class="sh">"</span><span class="s">/</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">page</span><span class="p">.</span><span class="n">title</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">revision</span> <span class="ow">in</span> <span class="n">page</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">revision</span><span class="p">.</span><span class="n">text</span> <span class="ow">or</span> <span class="sh">""</span>
            <span class="k">if</span> <span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">():</span>
                <span class="nf">save_draft_article</span><span class="p">(</span><span class="n">page</span><span class="p">.</span><span class="n">title</span><span class="p">,</span> <span class="n">revision</span><span class="p">.</span><span class="n">timestamp</span><span class="p">,</span> <span class="n">revision</span><span class="p">.</span><span class="n">contributor</span><span class="p">)</span>
</code></pre></div></div> <h3 id="data-balancing--preparation-for-modeling">Data Balancing &amp; Preparation for Modeling</h3> <p>The draft dataset is naturally imbalanced: some WikiProjects (e.g. Biography) dominate, while niche topics (e.g. Military History, Medicine) are underrepresented. To prepare the data:</p> <ul> <li> <p>We use multi-hot encoding for WikiProject tags (multi-label problem)</p> </li> <li> <p>We apply undersampling of majority labels and oversampling of minority labels using a controlled hybrid approach</p> </li> <li> <p>We extract features using:</p> </li> <li> <p>Bag-of-Words (TF-IDF)</p> </li> <li> <p>FastText embeddings averaged over article tokens</p> </li> </ul> <p>We split the dataset into stratified train/test folds while preserving label distribution.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MultiLabelBinarizer</span>

<span class="n">mlb</span> <span class="o">=</span> <span class="nc">MultiLabelBinarizer</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mlb</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">draft_labels</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <h3 id="model-overview">Model Overview</h3> <p>We trained a multilabel classifier to predict topics for new Wikipedia drafts using:</p> <ul> <li> <strong>Text features</strong>: Bag-of-words and fastText word vectors</li> <li> <strong>Labels</strong>: Community-curated WikiProject categories</li> <li> <strong>Classifiers</strong>: Logistic regression, one-vs-rest models</li> </ul> <h4 id="code-snippet-baseline-model-with-scikit-learn">Code snippet: Baseline model with Scikit-learn</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.multiclass</span> <span class="kn">import</span> <span class="n">OneVsRestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="c1"># Preprocess draft text
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">draft_texts</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">draft_labels</span>  <span class="c1"># multi-hot encoded WikiProject tags
</span>
<span class="c1"># Train classifier
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">OneVsRestClassifier</span><span class="p">(</span><span class="nc">LogisticRegression</span><span class="p">())</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <h3 id="-evaluation-metrics">üìä Evaluation Metrics</h3> <p>To evaluate our topic prediction model, we use standard multilabel classification metrics:</p> <ul> <li> <strong>Precision</strong>: What proportion of predicted topics are actually correct?</li> <li> <strong>Recall</strong>: What proportion of relevant topics are successfully predicted?</li> <li> <strong>F1 Score</strong>: Harmonic mean of precision and recall, providing a balanced measure.</li> <li> <strong>Coverage Error</strong>: Measures how far down the ranked list of topics we have to go to cover all true labels.</li> <li> <strong>Subset Accuracy</strong>: Percentage of examples where all predicted labels exactly match the true set (strict metric).</li> </ul> <p>We report both <strong>macro</strong> and <strong>micro</strong> versions of these metrics:</p> <ul> <li> <strong>Macro</strong> averages over all labels equally, highlighting performance on rare topics.</li> <li> <strong>Micro</strong> aggregates over all instances, reflecting overall system performance.</li> </ul> <p>Below is the precision-recall (PR) curve for our best-performing model across all 43 topic categories:</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pr_curve_drafttopic-480.webp 480w,/assets/img/pr_curve_drafttopic-800.webp 800w,/assets/img/pr_curve_drafttopic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/pr_curve_drafttopic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Method Overview" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Precision recall curve. </div> <p>The curve shows high performance in high-confidence regions, indicating the model‚Äôs utility in supporting triage through confidence-based routing.</p> <hr> <h3 id="-insights-from-topic-prediction-on-drafts">üîç Insights from Topic Prediction on Drafts</h3> <p>Our analysis surfaced several key takeaways:</p> <ul> <li> <strong>Early drafts contain rich topical signals</strong> ‚Äì Even just the opening sentences of a draft can be used to reliably predict WikiProject-level topics.</li> <li> <strong>Wikipedia‚Äôs folksonomies are machine-actionable</strong> ‚Äì Despite their grassroots origins, WikiProjects form a surprisingly consistent label set that works well for ML tasks after normalization.</li> <li> <strong>Topic predictability varies by category</strong> ‚Äì Technical domains (e.g., ‚ÄúComputing‚Äù, ‚ÄúMedicine‚Äù) are easier to identify, while broad domains (e.g., ‚ÄúCulture‚Äù, ‚ÄúSociety‚Äù) show significant overlap and ambiguity.</li> <li> <strong>Model confidence is a useful signal</strong> ‚Äì High-confidence predictions align with clearer cases; low-confidence predictions often highlight time-consuming judgment calls (TCJCs).</li> </ul> <p>These insights can help reviewers better triage new drafts‚Äîby prioritizing ambiguous, low-confidence drafts that most need expert attention.</p> <hr> <h3 id="-future-directions">üöÄ Future Directions</h3> <p>This work opens up several avenues for tooling and research:</p> <ul> <li> <strong>Reviewer Routing Interfaces</strong>: Develop UI tools to route drafts based on predicted topic and model confidence to the most appropriate WikiProject reviewers.</li> <li> <strong>Interactive Topic Feedback</strong>: Allow reviewers to adjust model-predicted topics, creating a human-in-the-loop system that continuously improves via feedback.</li> <li> <strong>Ontology-Aware Modeling</strong>: Incorporate the hierarchical structure of WikiProjects (e.g., <code class="language-plaintext highlighter-rouge">Biography &gt; Scientists &gt; Biologists</code>) to support more granular and semantically aware predictions.</li> <li> <strong>Explainable AI Integration</strong>: Highlight influential words or phrases in the draft that led to a given topic prediction to improve reviewer trust and transparency.</li> <li> <strong>Tooling for Draft Backlogs</strong>: Integrate predictions into the Page Curation Tool and AfC dashboards to offer suggested topics for new drafts at scale.</li> </ul> <p>Together, these next steps can move Wikipedia‚Äôs article review from reactive backlog management to proactive, intelligent triage.</p> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Sumit Asthana. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>